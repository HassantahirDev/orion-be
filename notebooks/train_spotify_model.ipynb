{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spotify Track Success Predictor - Model Training\n",
        "\n",
        "This notebook trains a Logistic Regression model to predict whether a Spotify track is likely to be successful (popularity â‰¥ 70).\n",
        "\n",
        "The trained model will be saved as JSON files that can be loaded directly by the NestJS backend.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import json\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 114000 tracks\n",
            "\n",
            "Dataset shape: (114000, 21)\n",
            "\n",
            "Columns: ['Unnamed: 0', 'track_id', 'artists', 'album_name', 'track_name', 'popularity', 'duration_ms', 'explicit', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature', 'track_genre']\n"
          ]
        }
      ],
      "source": [
        "# Paths - adjust BASE_DIR based on your notebook location\n",
        "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath('')))\n",
        "DATA_PATH = os.path.join(BASE_DIR, 'backend', 'data', 'spotify_tracks.csv')\n",
        "MODEL_DIR = os.path.join(BASE_DIR, 'backend', 'model')\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(f\"Loaded {len(df)} tracks\")\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "print(f\"\\nColumns: {df.columns.tolist()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create Target Variable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success rate: 4.80%\n",
            "\n",
            "Success distribution:\n",
            "success\n",
            "0    108528\n",
            "1      5472\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Create target variable: success = 1 if popularity >= 70, else 0\n",
        "df['success'] = (df['popularity'] >= 70).astype(int)\n",
        "print(f\"Success rate: {df['success'].mean() * 100:.2f}%\")\n",
        "print(f\"\\nSuccess distribution:\")\n",
        "print(df['success'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Select Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features: ['danceability', 'energy', 'loudness', 'tempo', 'duration_ms']\n",
            "\n",
            "Feature statistics:\n",
            "        danceability         energy       loudness          tempo  \\\n",
            "count  114000.000000  114000.000000  114000.000000  114000.000000   \n",
            "mean        0.566800       0.641383      -8.258960     122.147837   \n",
            "std         0.173542       0.251529       5.029337      29.978197   \n",
            "min         0.000000       0.000000     -49.531000       0.000000   \n",
            "25%         0.456000       0.472000     -10.013000      99.218750   \n",
            "50%         0.580000       0.685000      -7.004000     122.017000   \n",
            "75%         0.695000       0.854000      -5.003000     140.071000   \n",
            "max         0.985000       1.000000       4.532000     243.372000   \n",
            "\n",
            "        duration_ms  \n",
            "count  1.140000e+05  \n",
            "mean   2.280292e+05  \n",
            "std    1.072977e+05  \n",
            "min    0.000000e+00  \n",
            "25%    1.740660e+05  \n",
            "50%    2.129060e+05  \n",
            "75%    2.615060e+05  \n",
            "max    5.237295e+06  \n"
          ]
        }
      ],
      "source": [
        "# Select features (matching your dataset columns)\n",
        "feature_columns = [\n",
        "    'danceability',\n",
        "    'energy',\n",
        "    'loudness',\n",
        "    'tempo',\n",
        "    'duration_ms'\n",
        "]\n",
        "\n",
        "X = df[feature_columns].copy()\n",
        "y = df['success'].copy()\n",
        "\n",
        "print(f\"Features: {feature_columns}\")\n",
        "print(f\"\\nFeature statistics:\")\n",
        "print(X.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Handle Missing Values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values: 0\n",
            "No missing values found.\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values\n",
        "missing_count = X.isnull().sum().sum()\n",
        "print(f\"Missing values: {missing_count}\")\n",
        "\n",
        "if missing_count > 0:\n",
        "    print(\"\\nFilling missing values with mean...\")\n",
        "    X = X.fillna(X.mean())\n",
        "    print(\"Missing values filled.\")\n",
        "else:\n",
        "    print(\"No missing values found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Split Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set: 91200 samples\n",
            "Test set: 22800 samples\n",
            "\n",
            "Training success rate: 4.80%\n",
            "Test success rate: 4.80%\n"
          ]
        }
      ],
      "source": [
        "# Split data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {len(X_train)} samples\")\n",
        "print(f\"Test set: {len(X_test)} samples\")\n",
        "print(f\"\\nTraining success rate: {y_train.mean() * 100:.2f}%\")\n",
        "print(f\"Test success rate: {y_test.mean() * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Scale Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features scaled using StandardScaler\n",
            "\n",
            "Scaler mean: [ 5.66932471e-01  6.41719801e-01 -8.24817505e+00  1.22199313e+02\n",
            "  2.27829386e+05]\n",
            "Scaler scale: [1.73615621e-01 2.51580857e-01 5.02223715e+00 2.99946370e+01\n",
            " 1.05553572e+05]\n"
          ]
        }
      ],
      "source": [
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Features scaled using StandardScaler\")\n",
        "print(f\"\\nScaler mean: {scaler.mean_}\")\n",
        "print(f\"Scaler scale: {scaler.scale_}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model trained successfully!\n",
            "\n",
            "Model coefficients: [ 0.22696789 -0.36867651  0.81913646 -0.1024276  -0.07037222]\n",
            "Model intercept: -3.1464639533904264\n"
          ]
        }
      ],
      "source": [
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Model trained successfully!\")\n",
        "print(f\"\\nModel coefficients: {model.coef_[0]}\")\n",
        "print(f\"Model intercept: {model.intercept_[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Evaluate Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ðŸ“Š Model Evaluation Metrics:\n",
            "============================================================\n",
            "   Accuracy:  0.9520\n",
            "   Precision: 0.0000\n",
            "   Recall:    0.0000\n",
            "   F1 Score:  0.0000\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ðŸ“Š Model Evaluation Metrics:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"   Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"   Precision: {precision:.4f}\")\n",
        "print(f\"   Recall:    {recall:.4f}\")\n",
        "print(f\"   F1 Score:  {f1:.4f}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Model as JSON (for NestJS)\n",
        "\n",
        "Save model parameters and scaler parameters as JSON files that can be loaded by the NestJS backend.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Model saved to: /Users/hassantahir/Desktop/final-project/backend/model/model.json\n",
            "âœ… Scaler saved to: /Users/hassantahir/Desktop/final-project/backend/model/scaler.json\n",
            "\n",
            "âœ… Model training completed successfully!\n",
            "\n",
            "The model is now ready to be used by the NestJS backend endpoints.\n"
          ]
        }
      ],
      "source": [
        "# Create model directory if it doesn't exist\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Prepare model data for JSON export\n",
        "model_data = {\n",
        "    \"coefficients\": model.coef_[0].tolist(),\n",
        "    \"intercept\": model.intercept_[0].item(),\n",
        "    \"feature_order\": feature_columns\n",
        "}\n",
        "\n",
        "scaler_data = {\n",
        "    \"mean\": scaler.mean_.tolist(),\n",
        "    \"scale\": scaler.scale_.tolist(),\n",
        "    \"feature_order\": feature_columns\n",
        "}\n",
        "\n",
        "# Save as JSON files\n",
        "MODEL_JSON_PATH = os.path.join(MODEL_DIR, 'model.json')\n",
        "SCALER_JSON_PATH = os.path.join(MODEL_DIR, 'scaler.json')\n",
        "\n",
        "with open(MODEL_JSON_PATH, 'w') as f:\n",
        "    json.dump(model_data, f, indent=2)\n",
        "\n",
        "with open(SCALER_JSON_PATH, 'w') as f:\n",
        "    json.dump(scaler_data, f, indent=2)\n",
        "\n",
        "print(f\"âœ… Model saved to: {MODEL_JSON_PATH}\")\n",
        "print(f\"âœ… Scaler saved to: {SCALER_JSON_PATH}\")\n",
        "print(\"\\nâœ… Model training completed successfully!\")\n",
        "print(\"\\nThe model is now ready to be used by the NestJS backend endpoints.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
